{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GPT on addition\n",
    "\n",
    "Train a GPT model on a dedicated addition dataset to see if a Transformer can learn to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2021 22:13:00 - INFO - absl -   Starting the local TPU driver.\n",
      "08/03/2021 22:13:00 - INFO - absl -   Unable to initialize backend 'tpu_driver': Not found: Unable to find driver in registry given worker: local://\n",
      "08/03/2021 22:13:01 - INFO - absl -   Unable to initialize backend 'tpu': Invalid argument: TpuPlatform is not available.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.default_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "from typing import Mapping\n",
    "import functools\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AdditionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns addition problems of up to some number of digits in the inputs. Recall\n",
    "    that all GPT cares about are sequences of integers, and completing them according to\n",
    "    patterns in the data. Therefore, we have to somehow encode addition problems\n",
    "    as a sequence of integers.\n",
    "    \n",
    "    The sum of two n-digit numbers gives a third up to (n+1)-digit number. So our\n",
    "    encoding will simply be the n-digit first number, n-digit second number, \n",
    "    and (n+1)-digit result, all simply concatenated together. Because each addition\n",
    "    problem is so structured, there is no need to bother the model with encoding\n",
    "    +, =, or other tokens. Each possible sequence has the same length, and simply\n",
    "    contains the raw digits of the addition problem.\n",
    "    \n",
    "    As a few examples, the 2-digit problems:\n",
    "    - 85 + 50 = 135 becomes the sequence [8, 5, 5, 0, 1, 3, 5]\n",
    "    - 6 + 39 = 45 becomes the sequence [0, 6, 3, 9, 0, 4, 5]\n",
    "    etc.\n",
    "    \n",
    "    We will also only train GPT on the final (n+1)-digits because the first\n",
    "    two n-digits are always assumed to be given. So when we give GPT an exam later,\n",
    "    we will e.g. feed it the sequence [0, 6, 3, 9], which encodes that we'd like\n",
    "    to add 6 + 39, and hope that the model completes the integer sequence with [0, 4, 5]\n",
    "    in 3 sequential steps.\n",
    "    \n",
    "    fun exercise: does it help if the result is asked to be produced in reverse order?\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ndigit, split):\n",
    "        self.split = split # train/test\n",
    "        self.ndigit = ndigit\n",
    "        self.vocab_size = 10 # 10 possible digits 0..9\n",
    "        # +1 due to potential carry overflow, but then -1 because very last digit doesn't plug back\n",
    "        self.block_size = ndigit + ndigit + ndigit + 1 - 1\n",
    "        \n",
    "        # split up all addition problems into either training data or test data\n",
    "        num = (10**self.ndigit)**2 # total number of possible combinations\n",
    "        r = np.random.RandomState(1337) # make deterministic\n",
    "        perm = r.permutation(num)\n",
    "        num_test = min(int(num*0.2), 1000) # 20% of the whole dataset, or only up to 1000\n",
    "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ixes.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # given a problem index idx, first recover the associated a + b\n",
    "        idx = self.ixes[idx]\n",
    "        nd = 10**self.ndigit\n",
    "        a = idx // nd\n",
    "        b = idx %  nd\n",
    "        c = a + b\n",
    "        render = f'%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d' % (a,b,c) # e.g. 03+25=28 becomes \"0325028\" \n",
    "        dix = [int(s) for s in render] # convert each character to its token index\n",
    "        # x will be input to GPT and y will be the associated expected outputs\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\n",
    "        y[:self.ndigit*2-1] = -100 # we will only train in the output locations. -100 will mask loss to zero\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset for e.g. 2-digit addition\n",
    "ndigit = 2\n",
    "train_dataset = AdditionDataset(ndigit=ndigit, split='train')\n",
    "test_dataset = AdditionDataset(ndigit=ndigit, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4, 7, 1, 7, 0, 6]), tensor([-100, -100, -100,    0,    6,    4]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0] # sample a training instance just to see what one raw example looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingpt.model import gpt, loss_fn, GPTConfig\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "# initialize a baby GPT model\n",
    "gpt_config = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, \n",
    "                  n_layer=2, n_head=4, n_embd=128)\n",
    "hk_loss_fn = hk.transform(partial(loss_fn, config=gpt_config, is_training=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "rng, subkey = jax.random.split(rng)\n",
    "tconf = TrainerConfig(max_epochs=50, batch_size=512//2, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=1024, final_tokens=50*len(train_dataset)*(ndigit+1),\n",
    "                      num_workers=4, rng=subkey, step_tokens=3)\n",
    "trainer = Trainer(hk_loss_fn, train_dataset, test_dataset, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2021 22:13:07 - INFO - mingpt.trainer -   number of parameters: 400128\n"
     ]
    }
   ],
   "source": [
    "params = trainer.init_params() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u/.anaconda/envs/mingpt/lib/python3.9/site-packages/jax/experimental/maps.py:500: UserWarning: xmap is an experimental feature and probably has bugs!\n",
      "  warn(\"xmap is an experimental feature and probably has bugs!\")\n",
      "epoch 1 iter 35: train loss 1.62870. lr 5.994563e-04: 100%|██████████| 36/36 [00:12<00:00,  2.78it/s]\n",
      "08/03/2021 22:13:22 - INFO - mingpt.trainer -   test loss: 1.6405513286590576\n",
      "epoch 2 iter 71: train loss 1.47196. lr 5.976741e-04: 100%|██████████| 36/36 [00:00<00:00, 37.12it/s]\n",
      "08/03/2021 22:13:24 - INFO - mingpt.trainer -   test loss: 1.4449920654296875\n",
      "epoch 3 iter 107: train loss 1.40264. lr 5.946582e-04: 100%|██████████| 36/36 [00:00<00:00, 36.59it/s]\n",
      "08/03/2021 22:13:25 - INFO - mingpt.trainer -   test loss: 1.332991361618042\n",
      "epoch 4 iter 143: train loss 1.22187. lr 5.904211e-04: 100%|██████████| 36/36 [00:01<00:00, 35.12it/s]\n",
      "08/03/2021 22:13:27 - INFO - mingpt.trainer -   test loss: 1.2509129047393799\n",
      "epoch 5 iter 179: train loss 1.17210. lr 5.849804e-04: 100%|██████████| 36/36 [00:00<00:00, 36.91it/s]\n",
      "08/03/2021 22:13:28 - INFO - mingpt.trainer -   test loss: 1.217337965965271\n",
      "epoch 6 iter 215: train loss 1.08468. lr 5.783586e-04: 100%|██████████| 36/36 [00:00<00:00, 36.47it/s]\n",
      "08/03/2021 22:13:30 - INFO - mingpt.trainer -   test loss: 1.171798825263977\n",
      "epoch 7 iter 251: train loss 1.14599. lr 5.705831e-04: 100%|██████████| 36/36 [00:00<00:00, 36.07it/s]\n",
      "08/03/2021 22:13:32 - INFO - mingpt.trainer -   test loss: 1.1198925971984863\n",
      "epoch 8 iter 287: train loss 1.07115. lr 5.616862e-04: 100%|██████████| 36/36 [00:00<00:00, 36.47it/s]\n",
      "08/03/2021 22:13:33 - INFO - mingpt.trainer -   test loss: 1.1268608570098877\n",
      "epoch 9 iter 323: train loss 1.05841. lr 5.517047e-04: 100%|██████████| 36/36 [00:00<00:00, 36.49it/s]\n",
      "08/03/2021 22:13:35 - INFO - mingpt.trainer -   test loss: 1.0750887393951416\n",
      "epoch 10 iter 359: train loss 1.03927. lr 5.406801e-04: 100%|██████████| 36/36 [00:00<00:00, 36.25it/s]\n",
      "08/03/2021 22:13:36 - INFO - mingpt.trainer -   test loss: 1.051440715789795\n",
      "epoch 11 iter 395: train loss 1.00031. lr 5.286580e-04: 100%|██████████| 36/36 [00:00<00:00, 36.05it/s]\n",
      "08/03/2021 22:13:38 - INFO - mingpt.trainer -   test loss: 1.0391112565994263\n",
      "epoch 12 iter 431: train loss 0.98816. lr 5.156882e-04: 100%|██████████| 36/36 [00:00<00:00, 36.58it/s]\n",
      "08/03/2021 22:13:40 - INFO - mingpt.trainer -   test loss: 0.9860973358154297\n",
      "epoch 13 iter 467: train loss 0.61565. lr 5.018245e-04: 100%|██████████| 36/36 [00:00<00:00, 36.02it/s]\n",
      "08/03/2021 22:13:41 - INFO - mingpt.trainer -   test loss: 0.6803889870643616\n",
      "epoch 14 iter 503: train loss 0.57581. lr 4.871244e-04: 100%|██████████| 36/36 [00:00<00:00, 36.00it/s]\n",
      "08/03/2021 22:13:43 - INFO - mingpt.trainer -   test loss: 0.48549047112464905\n",
      "epoch 15 iter 539: train loss 0.49145. lr 4.716487e-04: 100%|██████████| 36/36 [00:00<00:00, 36.23it/s]\n",
      "08/03/2021 22:13:44 - INFO - mingpt.trainer -   test loss: 0.43637409806251526\n",
      "epoch 16 iter 575: train loss 0.54901. lr 4.554616e-04: 100%|██████████| 36/36 [00:00<00:00, 36.88it/s]\n",
      "08/03/2021 22:13:46 - INFO - mingpt.trainer -   test loss: 0.4147152602672577\n",
      "epoch 17 iter 611: train loss 0.43722. lr 4.386302e-04: 100%|██████████| 36/36 [00:00<00:00, 36.32it/s]\n",
      "08/03/2021 22:13:47 - INFO - mingpt.trainer -   test loss: 0.4745672643184662\n",
      "epoch 18 iter 647: train loss 0.49992. lr 4.212243e-04: 100%|██████████| 36/36 [00:01<00:00, 35.62it/s]\n",
      "08/03/2021 22:13:49 - INFO - mingpt.trainer -   test loss: 0.4300333857536316\n",
      "epoch 19 iter 683: train loss 0.46698. lr 4.033159e-04: 100%|██████████| 36/36 [00:00<00:00, 36.87it/s]\n",
      "08/03/2021 22:13:51 - INFO - mingpt.trainer -   test loss: 0.37481045722961426\n",
      "epoch 20 iter 719: train loss 0.39539. lr 3.849794e-04: 100%|██████████| 36/36 [00:00<00:00, 37.87it/s]\n",
      "08/03/2021 22:13:52 - INFO - mingpt.trainer -   test loss: 0.4385663568973541\n",
      "epoch 21 iter 755: train loss 0.26698. lr 3.662907e-04: 100%|██████████| 36/36 [00:01<00:00, 35.92it/s]\n",
      "08/03/2021 22:13:54 - INFO - mingpt.trainer -   test loss: 0.3825054168701172\n",
      "epoch 22 iter 791: train loss 0.50670. lr 3.473272e-04: 100%|██████████| 36/36 [00:00<00:00, 36.91it/s]\n",
      "08/03/2021 22:13:55 - INFO - mingpt.trainer -   test loss: 0.32754379510879517\n",
      "epoch 23 iter 827: train loss 0.19772. lr 3.281676e-04: 100%|██████████| 36/36 [00:00<00:00, 36.73it/s]\n",
      "08/03/2021 22:13:57 - INFO - mingpt.trainer -   test loss: 0.29703041911125183\n",
      "epoch 24 iter 863: train loss 0.45220. lr 3.088913e-04: 100%|██████████| 36/36 [00:00<00:00, 36.89it/s]\n",
      "08/03/2021 22:13:59 - INFO - mingpt.trainer -   test loss: 0.3424426019191742\n",
      "epoch 25 iter 899: train loss 0.22699. lr 2.895781e-04: 100%|██████████| 36/36 [00:00<00:00, 37.13it/s]\n",
      "08/03/2021 22:14:00 - INFO - mingpt.trainer -   test loss: 0.3078526258468628\n",
      "epoch 26 iter 935: train loss 0.80693. lr 2.703081e-04: 100%|██████████| 36/36 [00:00<00:00, 36.80it/s]\n",
      "08/03/2021 22:14:02 - INFO - mingpt.trainer -   test loss: 0.4295543134212494\n",
      "epoch 27 iter 971: train loss 0.38067. lr 2.511611e-04: 100%|██████████| 36/36 [00:00<00:00, 37.44it/s]\n",
      "08/03/2021 22:14:03 - INFO - mingpt.trainer -   test loss: 0.24994336068630219\n",
      "epoch 28 iter 1007: train loss 0.16257. lr 2.322166e-04: 100%|██████████| 36/36 [00:01<00:00, 33.71it/s]\n",
      "08/03/2021 22:14:05 - INFO - mingpt.trainer -   test loss: 0.24894680082798004\n",
      "epoch 29 iter 1043: train loss 0.13850. lr 2.135530e-04: 100%|██████████| 36/36 [00:00<00:00, 36.57it/s]\n",
      "08/03/2021 22:14:07 - INFO - mingpt.trainer -   test loss: 0.3271248936653137\n",
      "epoch 30 iter 1079: train loss 0.26535. lr 1.952476e-04: 100%|██████████| 36/36 [00:00<00:00, 36.74it/s]\n",
      "08/03/2021 22:14:08 - INFO - mingpt.trainer -   test loss: 0.35906222462654114\n",
      "epoch 31 iter 1115: train loss 0.16472. lr 1.773765e-04: 100%|██████████| 36/36 [00:00<00:00, 36.05it/s]\n",
      "08/03/2021 22:14:10 - INFO - mingpt.trainer -   test loss: 0.23044778406620026\n",
      "epoch 32 iter 1151: train loss 0.16015. lr 1.600134e-04: 100%|██████████| 36/36 [00:01<00:00, 31.88it/s]\n",
      "08/03/2021 22:14:11 - INFO - mingpt.trainer -   test loss: 0.1533021181821823\n",
      "epoch 33 iter 1187: train loss 0.14102. lr 1.432306e-04: 100%|██████████| 36/36 [00:00<00:00, 36.84it/s]\n",
      "08/03/2021 22:14:13 - INFO - mingpt.trainer -   test loss: 0.3183065354824066\n",
      "epoch 34 iter 1223: train loss 0.17490. lr 1.270975e-04: 100%|██████████| 36/36 [00:00<00:00, 36.43it/s]\n",
      "08/03/2021 22:14:15 - INFO - mingpt.trainer -   test loss: 0.2351905107498169\n",
      "epoch 35 iter 1259: train loss 0.43615. lr 1.116811e-04: 100%|██████████| 36/36 [00:00<00:00, 37.12it/s]\n",
      "08/03/2021 22:14:16 - INFO - mingpt.trainer -   test loss: 0.17515307664871216\n",
      "epoch 36 iter 1295: train loss 0.19551. lr 9.704508e-05: 100%|██████████| 36/36 [00:00<00:00, 36.34it/s]\n",
      "08/03/2021 22:14:18 - INFO - mingpt.trainer -   test loss: 0.18931016325950623\n",
      "epoch 37 iter 1331: train loss 0.11556. lr 8.325014e-05: 100%|██████████| 36/36 [00:00<00:00, 36.47it/s]\n",
      "08/03/2021 22:14:19 - INFO - mingpt.trainer -   test loss: 0.1661672741174698\n",
      "epoch 38 iter 1367: train loss 0.13943. lr 7.035359e-05: 100%|██████████| 36/36 [00:01<00:00, 35.63it/s]\n",
      "08/03/2021 22:14:21 - INFO - mingpt.trainer -   test loss: 0.2341461032629013\n",
      "epoch 39 iter 1403: train loss 0.12435. lr 6.000000e-05: 100%|██████████| 36/36 [00:00<00:00, 36.37it/s]\n",
      "08/03/2021 22:14:23 - INFO - mingpt.trainer -   test loss: 0.25328898429870605\n",
      "epoch 40 iter 1439: train loss 0.13401. lr 6.000000e-05: 100%|██████████| 36/36 [00:00<00:00, 37.35it/s]\n",
      "08/03/2021 22:14:24 - INFO - mingpt.trainer -   test loss: 0.07525189965963364\n",
      "epoch 41 iter 1475: train loss 0.08913. lr 6.000000e-05: 100%|██████████| 36/36 [00:01<00:00, 35.89it/s]\n",
      "08/03/2021 22:14:26 - INFO - mingpt.trainer -   test loss: 0.14591340720653534\n",
      "epoch 42 iter 1511: train loss 0.34530. lr 6.000000e-05: 100%|██████████| 36/36 [00:00<00:00, 37.92it/s]\n",
      "08/03/2021 22:14:27 - INFO - mingpt.trainer -   test loss: 0.17748385667800903\n",
      "epoch 43 iter 1547: train loss 0.09890. lr 6.000000e-05: 100%|██████████| 36/36 [00:00<00:00, 36.23it/s]\n",
      "08/03/2021 22:14:29 - INFO - mingpt.trainer -   test loss: 0.10846535861492157\n",
      "epoch 44 iter 1583: train loss 0.08910. lr 6.000000e-05: 100%|██████████| 36/36 [00:00<00:00, 37.02it/s]\n",
      "08/03/2021 22:14:30 - INFO - mingpt.trainer -   test loss: 0.1119178757071495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 45 iter 1619: train loss 0.11055. lr 6.000000e-05: 100%|██████████| 36/36 [00:00<00:00, 36.62it/s]\n",
      "08/03/2021 22:14:32 - INFO - mingpt.trainer -   test loss: 0.18536904454231262\n",
      "epoch 46 iter 1655: train loss 0.04970. lr 6.000000e-05: 100%|██████████| 36/36 [00:00<00:00, 36.32it/s]\n",
      "08/03/2021 22:14:34 - INFO - mingpt.trainer -   test loss: 0.05911295861005783\n",
      "epoch 47 iter 1691: train loss 0.08616. lr 6.000000e-05: 100%|██████████| 36/36 [00:00<00:00, 36.33it/s]\n",
      "08/03/2021 22:14:35 - INFO - mingpt.trainer -   test loss: 0.1462254822254181\n",
      "epoch 48 iter 1727: train loss 0.04438. lr 6.000000e-05: 100%|██████████| 36/36 [00:00<00:00, 36.65it/s]\n",
      "08/03/2021 22:14:37 - INFO - mingpt.trainer -   test loss: 0.13501769304275513\n",
      "epoch 49 iter 1763: train loss 0.04471. lr 6.000000e-05: 100%|██████████| 36/36 [00:01<00:00, 35.38it/s]\n",
      "08/03/2021 22:14:38 - INFO - mingpt.trainer -   test loss: 0.1492648869752884\n",
      "epoch 50 iter 1799: train loss 0.08964. lr 6.000000e-05: 100%|██████████| 36/36 [00:01<00:00, 35.98it/s]\n",
      "08/03/2021 22:14:40 - INFO - mingpt.trainer -   test loss: 0.21571335196495056\n"
     ]
    }
   ],
   "source": [
    "params, _ = trainer.train(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's give the trained model an addition exam\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.utils import sample\n",
    "\n",
    "model = hk.transform(partial(gpt, config=gpt_config, is_training=False))\n",
    "model = hk.without_apply_rng(model).apply\n",
    "\n",
    "def give_exam(dataset, batch_size=32, max_batches=-1):\n",
    "    batch_size=1024\n",
    "    max_batches=10\n",
    "    results = []\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    for b, (x, y) in enumerate(loader):\n",
    "        d1d2 = jnp.array(x[:, :ndigit*2])\n",
    "        batched_sample = partial(sample, params=params, model=model, config=gpt_config, steps=ndigit+1)\n",
    "        d1d2d3 = jax.vmap(batched_sample)(x=jnp.array(d1d2))\n",
    "        #d1d2d3 = sample(params, model, gpt_config, d1d2, ndigit+1\n",
    "        d3 = d1d2d3[:, -(ndigit+1):]\n",
    "        factors = jnp.array([[10**i for i in range(ndigit+1)][::-1]])\n",
    "        # decode the integers from individual digits\n",
    "        d1i = (d1d2[:,:ndigit] * factors[:,1:]).sum(1)\n",
    "        d2i = (d1d2[:,ndigit:ndigit*2] * factors[:,1:]).sum(1)\n",
    "        d3i_pred = (d3 * factors).sum(1)\n",
    "        d3i_gt = d1i + d2i\n",
    "        correct = (d3i_pred == d3i_gt) # Software 1.0 vs. Software 2.0 fight RIGHT on this line, lol\n",
    "        for i in range(x.size(0)):\n",
    "            results.append(int(correct[i]))\n",
    "            judge = 'YEP!!!' if correct[i] else 'NOPE'\n",
    "            if not correct[i]:\n",
    "                print(\"GPT claims that %03d + %03d = %03d (gt is %03d; %s)\" \n",
    "                      % (d1i[i], d2i[i], d3i_pred[i], d3i_gt[i], judge))\n",
    "        \n",
    "        if max_batches >= 0 and b+1 >= max_batches:\n",
    "            break\n",
    "    print(\"final score: %d/%d = %.2f%% correct\" % (np.sum(results), len(results), 100*np.mean(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT claims that 069 + 031 = 090 (gt is 100; NOPE)\n",
      "GPT claims that 009 + 091 = 090 (gt is 100; NOPE)\n",
      "GPT claims that 099 + 001 = 090 (gt is 100; NOPE)\n",
      "GPT claims that 089 + 011 = 090 (gt is 100; NOPE)\n",
      "GPT claims that 005 + 015 = 010 (gt is 020; NOPE)\n",
      "GPT claims that 005 + 005 = 000 (gt is 010; NOPE)\n",
      "GPT claims that 005 + 085 = 080 (gt is 090; NOPE)\n",
      "GPT claims that 025 + 005 = 020 (gt is 030; NOPE)\n",
      "GPT claims that 045 + 005 = 040 (gt is 050; NOPE)\n",
      "GPT claims that 019 + 081 = 090 (gt is 100; NOPE)\n",
      "GPT claims that 005 + 055 = 050 (gt is 060; NOPE)\n",
      "GPT claims that 005 + 035 = 030 (gt is 040; NOPE)\n",
      "GPT claims that 005 + 075 = 070 (gt is 080; NOPE)\n",
      "GPT claims that 005 + 025 = 020 (gt is 030; NOPE)\n",
      "GPT claims that 015 + 005 = 010 (gt is 020; NOPE)\n",
      "GPT claims that 075 + 005 = 070 (gt is 080; NOPE)\n",
      "GPT claims that 005 + 045 = 040 (gt is 050; NOPE)\n",
      "GPT claims that 079 + 021 = 090 (gt is 100; NOPE)\n",
      "GPT claims that 085 + 005 = 080 (gt is 090; NOPE)\n",
      "final score: 8981/9000 = 99.79% correct\n"
     ]
    }
   ],
   "source": [
    "# training set: how well did we memorize?\n",
    "give_exam(train_dataset, batch_size=1024, max_batches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 1000/1000 = 100.00% correct\n"
     ]
    }
   ],
   "source": [
    "# test set: how well did we generalize?\n",
    "give_exam(test_dataset, batch_size=1024, max_batches=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that's amusing... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mingpt]",
   "language": "python",
   "name": "conda-env-mingpt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
